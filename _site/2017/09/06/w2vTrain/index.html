<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Boutique consulting -- AI & Data Analytics.">

    <title>Word2Vec Model Training - Sparkling Data Ocean</title>

    <link rel="canonical" href="http://localhost:4000/2017/09/06/w2vTrain/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Sparkling Data Ocean" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-114694347-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-114694347-1');
    </script>

</head>


<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Sparkling Data Ocean</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="/">Home</a>
                </li>
                
				
                <li>
                    <a href="/about/">About</a>
                </li>
				
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Post Header -->
<header class="intro-header" style="background-image: url('/img/mod10.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>Word2Vec Model Training</h1>
                    
                    <h2 class="subheading">Playing with Word2vec Model</h2>
                    
                    <span class="meta">Posted by Melenar on September 6, 2017</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

				<h3>About Word2Vec Model </h3>
<p><i><a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec model</a></i> was created by a team lead by Tomas Mikolov in Google. In 2015 is became an open source product.
Word2Vec model transforms words to vectors which gives us new insights in text analytics.
Here is an excellent article about word2vec model:
<i><a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">The amazing power of word vectors</a></i>.</p>

<p>In our posts we will introduce a new Word2Vec2Graph model - a model that combines Word2Vec and graph functionalities. We will build graphs using words as nodes and Word2Vec cosine similarities and edge weights. Word2Vec graphs will give us new insights like top words in text file - pageRank, word topics - connected components, word neighbors - 'find' function. </p>
<p>Let's look at some examples of Word2Vec2Graph model based on text that describes Word2Vec model. We'll start with well known algorithm - Google pageRank. Here are top pageRank words that shows us then Word2Vec model is about words, vectors, training, and so on: </p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">id</span><span class="o">,</span><span class="n">pagerank</span>
<span class="n">words</span><span class="o">,</span><span class="mf">28.97</span>
<span class="n">model</span><span class="o">,</span><span class="mf">25.08</span>
<span class="n">vectors</span><span class="o">,</span><span class="mf">20.24</span>
<span class="n">training</span><span class="o">,</span><span class="mf">16.32</span>
<span class="n">vector</span><span class="o">,</span><span class="mf">15.32</span>
<span class="n">using</span><span class="o">,</span><span class="mf">14.31</span>
<span class="n">models</span><span class="o">,</span><span class="mf">13.84</span>
<span class="n">representations</span><span class="o">,</span><span class="mf">9.11</span>
<span class="n">example</span><span class="o">,</span><span class="mf">8.28</span>
<span class="n">syntactic</span><span class="o">,</span><span class="mf">8.21</span>
<span class="n">semantic</span><span class="o">,</span><span class="mf">8.00</span>
<span class="n">accuracy</span><span class="o">,</span><span class="mf">7.88</span>
<span class="n">results</span><span class="o">,</span><span class="mf">7.23</span>
<span class="n">phrases</span><span class="o">,</span><span class="mf">6.82</span>
<span class="n">vocabulary</span><span class="o">,</span><span class="mf">6.44</span>
<span class="n">neural</span><span class="o">,</span><span class="mf">6.03</span>
<span class="n">similar</span><span class="o">,</span><span class="mf">6.01</span>
<span class="n">context</span><span class="o">,</span><span class="mf">5.97</span></code></pre></figure>

<p>Spark GraphFrames 'find' function shows us which words in documents about Word2Vec model are located between the words 'words' and 'vectors'?</p>

<p><a href="#">
    <img src="/img/graphWv.jpg" alt="Post Sample Image" width="600" />
</a></p>

<p>The next few graphs demonstrate one of well known examples about the Word2Vec model: Country - Capital associations like France - Germany + Berlin = Paris: </p>

<p><a href="#">
    <img src="/img/countryCapital.jpg" alt="Post Sample Image" height="600" />
</a></p>

<p>The first picture shows connected component, the second 'Germany' neighbors and neighbors of neighbors, the third a circle of word pairs. Numbers on edges are Word2Vec cosine similarities between the words.</p>

<p>Here are some more examples. We built a graph of words with low Word2Vec cosine similarities, ran connected components (first picture) and looked at neighbors of neighbors for the word 'vectors' (second picture):</p>

<p><a href="#">
    <img src="/img/ccLowCos.jpg" alt="Post Sample Image" width="500" />
</a></p>

<p><a href="#">
    <img src="/img/ngbVectors.jpg" alt="Post Sample Image" width="500" />
</a></p>

<p>In the next several posts we will show how to build and use the Word2Vec2Graph model. As a tool we will use Spark. We will run it on Amazon Cloud via Databricks Community. </p>

<p><h3>Why Spark?</h3>
Until recently there were no single processing framework that was able to solve several very different analytical problems like statistics and graphs. Spark is the first framework that can do it. It is the fundamental advantage of Spark that provides a framework for advanced analytics right out of the box. This framework includes a tool for accelerated queries, a machine learning library, and graph processing engine.</p>
<p>
<h3>Databricks Community </h3>
<i><a href="https://databricks.com/blog/2016/02/17/introducing-databricks-community-edition-apache-spark-for-all.html">Databricks community edition</a></i> is an entry to Spark Big Data Analytics. It allows to create a cluster on Amazon Cloud and makes it is easy for data scientists and data engineers to write Spark code and debug it.
And it's free!</p>

<h3>Training a Word2Vec Model </h3>
<p>In our first post we will train Word2vec model in Spark and show how training corpus affects the Word2Vec model results.</p>
<p>AWS cluster that we run via Databricks Community is not so big. To be able to train Word2vec model we will get a 42 MB public file about news and load it to Databricks: </p>

<p></p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">inputNews</span><span class="k">=</span><span class="n">sc</span><span class="o">.</span>
  <span class="nf">textFile</span><span class="o">(</span><span class="s">"/FileStore/tables/newsTest.txt"</span><span class="o">).</span>
  <span class="nf">toDF</span><span class="o">(</span><span class="s">"charLine"</span><span class="o">)</span></code></pre></figure>

<p>First we'll tokenize the data</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.spark.ml._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.ml.feature._</span>
<span class="n">val</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">RegexTokenizer</span><span class="o">().</span>
   <span class="n">setInputCol</span><span class="o">(</span><span class="s">"charLine"</span><span class="o">).</span>
   <span class="n">setOutputCol</span><span class="o">(</span><span class="s">"value"</span><span class="o">).</span>
   <span class="n">setPattern</span><span class="o">(</span><span class="s">"[^a-z]+"</span><span class="o">).</span>
   <span class="n">setMinTokenLength</span><span class="o">(</span><span class="mi">3</span><span class="o">).</span>
   <span class="n">setGaps</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
<span class="n">val</span> <span class="n">tokenizedNews</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="na">transform</span><span class="o">(</span><span class="n">inputNews</span><span class="o">)</span></code></pre></figure>

<p>Then we'll train the Word2VecModel </p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kn">import</span> <span class="nn">org.apache.spark.ml.feature.Word2Vec</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.ml._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.ml.feature.Word2VecModel</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.Row</span>
<span class="n">val</span> <span class="n">word2vec</span><span class="o">=</span> <span class="k">new</span> <span class="nc">Word2Vec</span><span class="o">().</span>
   <span class="n">setInputCol</span><span class="o">(</span><span class="s">"value"</span><span class="o">).</span>
   <span class="n">setOutputCol</span><span class="o">(</span><span class="s">"result"</span><span class="o">)</span>
<span class="n">val</span> <span class="n">w2VmodelNews</span><span class="o">=</span><span class="n">word2vec</span><span class="o">.</span><span class="na">fit</span><span class="o">(</span><span class="n">tokenizedNews</span><span class="o">)</span></code></pre></figure>

<p>Then we will save the model and we don't need to train it again.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">w2VmodelNews</span><span class="o">.</span>
   <span class="n">write</span><span class="o">.</span>
   <span class="n">overwrite</span><span class="o">.</span>
   <span class="nf">save</span><span class="o">(</span><span class="s">"w2vNews"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">word2vecNews</span><span class="k">=</span><span class="nc">Word2VecModel</span><span class="o">.</span>
   <span class="n">read</span><span class="o">.</span>
   <span class="nf">load</span><span class="o">(</span><span class="s">"w2vNews"</span><span class="o">)</span></code></pre></figure>

<p>Now let's test the model. The most popular function of Word2Vec model shows us how different words are associated:</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nf">display</span><span class="o">(</span><span class="nv">word2vecNews</span><span class="o">.</span><span class="py">findSynonyms</span><span class="o">(</span><span class="s">"stress"</span><span class="o">,</span><span class="mi">7</span><span class="o">))</span>

<span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">risk</span><span class="o">,</span><span class="mf">0.6505142450332642</span>
<span class="n">adversely</span><span class="o">,</span><span class="mf">0.6353756785392761</span>
<span class="n">clots</span><span class="o">,</span><span class="mf">0.6308229565620422</span>
<span class="n">anxiety</span><span class="o">,</span><span class="mf">0.6186497807502747</span>
<span class="n">traumatic</span><span class="o">,</span><span class="mf">0.6167819499969482</span>
<span class="n">persistent</span><span class="o">,</span><span class="mf">0.6142207980155945</span>
<span class="n">problems</span><span class="o">,</span><span class="mf">0.6132286190986633</span></code></pre></figure>

<p><h3>How Trained Corpus Affects the Word2Vec Model? </h3>

To see how the corpus that we used to train the model affects the results we will add a small file, train the model on  combined corpus and compare the results. </p>
<p>To play with data about psychology we copied it from several Wikipedia articles, got a small file (180 KB), and combined it with news file (42 MB). Then we trained the Word2vec model on this combined file. </p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">inputWiki</span><span class="k">=</span><span class="nv">sc</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="s">"/FileStore/tables/WikiTest.txt"</span><span class="o">).</span>
   <span class="nf">toDF</span><span class="o">(</span><span class="s">"charLine"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">tokenizedNewsWiki</span> <span class="k">=</span> <span class="n">tokenizer</span><span class="o">.</span>
   <span class="nf">transform</span><span class="o">(</span><span class="n">inputNews</span><span class="o">.</span>
   <span class="nf">union</span><span class="o">(</span><span class="n">inputWiki</span><span class="o">))</span></code></pre></figure>

<p>Train Word2Vec model and save the results:</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="nv">w2VmodelNewsWiki</span><span class="k">=</span><span class="n">word2vec</span><span class="o">.</span>
   <span class="nf">fit</span><span class="o">(</span><span class="n">tokenizedNewsWiki</span><span class="o">)</span>
<span class="n">w2VmodelNewsWiki</span><span class="o">.</span>
   <span class="n">write</span><span class="o">.</span>
   <span class="n">overwrite</span><span class="o">.</span>
   <span class="nf">save</span><span class="o">(</span><span class="s">"w2vNewsWiki"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">modelNewsWiki</span><span class="k">=</span><span class="nc">Word2VecModel</span><span class="o">.</span>
   <span class="n">read</span><span class="o">.</span>
   <span class="nf">load</span><span class="o">(</span><span class="s">"w2vNewsWiki"</span><span class="o">)</span>
<span class="nf">display</span><span class="o">(</span><span class="n">modelNewsWiki</span><span class="o">.</span>
   <span class="nf">findSynonyms</span><span class="o">(</span><span class="s">"stress"</span><span class="o">,</span><span class="mi">7</span><span class="o">))</span></code></pre></figure>

<p>The results of these models are very different for some words and very similar for some other words. Here are examples:</p>

<p>Word: <b>Stress</b> - Input File: <b>News:</b></p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">risk</span><span class="o">,</span><span class="mf">0.6505142450332642</span>
<span class="n">adversely</span><span class="o">,</span><span class="mf">0.6353756785392761</span>
<span class="n">clots</span><span class="o">,</span><span class="mf">0.6308229565620422</span>
<span class="n">anxiety</span><span class="o">,</span><span class="mf">0.6186497807502747</span>
<span class="n">traumatic</span><span class="o">,</span><span class="mf">0.6167819499969482</span>
<span class="n">persistent</span><span class="o">,</span><span class="mf">0.6142207980155945</span>
<span class="n">problems</span><span class="o">,</span><span class="mf">0.6132286190986633</span></code></pre></figure>

<p>Input File: <b>News + Wiki</b>:</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">obesity</span><span class="o">,</span><span class="mf">0.6602367758750916</span>
<span class="n">adverse</span><span class="o">,</span><span class="mf">0.6559499502182007</span>
<span class="n">systemic</span><span class="o">,</span><span class="mf">0.6525574326515198</span>
<span class="n">averse</span><span class="o">,</span><span class="mf">0.6500416994094849</span>
<span class="n">risk</span><span class="o">,</span><span class="mf">0.6457705497741699</span>
<span class="n">detect</span><span class="o">,</span><span class="mf">0.6430484652519226</span>
<span class="n">infection</span><span class="o">,</span><span class="mf">0.6407146453857422</span></code></pre></figure>

<p>Word: <b>Rain</b> - Input File: <b>News:</b></p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">snow</span><span class="o">,</span><span class="mf">0.8456688523292542</span>
<span class="n">winds</span><span class="o">,</span><span class="mf">0.800561785697937</span>
<span class="n">rains</span><span class="o">,</span><span class="mf">0.7878957986831665</span>
<span class="n">fog</span><span class="o">,</span><span class="mf">0.7052807211875916</span>
<span class="n">inches</span><span class="o">,</span><span class="mf">0.690990686416626</span>
<span class="n">storm</span><span class="o">,</span><span class="mf">0.6725252270698547</span>
<span class="n">gusts</span><span class="o">,</span><span class="mf">0.6721619963645935</span></code></pre></figure>

<p>Input File: <b>News + Wiki:</b></p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="nc">Rain</span> <span class="nc">News</span><span class="o">/</span><span class="nc">Wiki</span>
<span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">snow</span><span class="o">,</span><span class="mf">0.8400915265083313</span>
<span class="n">rains</span><span class="o">,</span><span class="mf">0.7938879728317261</span>
<span class="n">winds</span><span class="o">,</span><span class="mf">0.7620705366134644</span>
<span class="n">mph</span><span class="o">,</span><span class="mf">0.7246450781822205</span>
<span class="n">storm</span><span class="o">,</span><span class="mf">0.7209596633911133</span>
<span class="n">storms</span><span class="o">,</span><span class="mf">0.7147307395935059</span>
<span class="n">inches</span><span class="o">,</span><span class="mf">0.7076087594032288</span></code></pre></figure>

<p>Word: <b>Specialty</b> - Input File: <b>News:</b></p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">semiconductor</span><span class="o">,</span><span class="mf">0.8236984014511108</span>
<span class="n">diversified</span><span class="o">,</span><span class="mf">0.8118916153907776</span>
<span class="n">biotech</span><span class="o">,</span><span class="mf">0.8052045106887817</span>
<span class="n">manufacturer</span><span class="o">,</span><span class="mf">0.789034903049469</span>
<span class="n">maxx</span><span class="o">,</span><span class="mf">0.7876819968223572</span>
<span class="n">boutiques</span><span class="o">,</span><span class="mf">0.785348117351532</span>
<span class="n">biotech</span></code></pre></figure>

<p>Input File: <b>News + Wiki:</b></p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">word</span><span class="o">,</span><span class="n">similarity</span>
<span class="n">diversified</span><span class="o">,</span><span class="mf">0.8359127640724182</span>
<span class="n">titan</span><span class="o">,</span><span class="mf">0.8055083751678467</span>
<span class="n">automation</span><span class="o">,</span><span class="mf">0.8038058876991272</span>
<span class="n">machinery</span><span class="o">,</span><span class="mf">0.8027305603027344</span>
<span class="n">computerized</span><span class="o">,</span><span class="mf">0.8011659383773804</span>
<span class="n">analytics</span><span class="o">,</span><span class="mf">0.8006263375282288</span>
<span class="n">apparel</span><span class="o">,</span><span class="mf">0.7975579500198364</span></code></pre></figure>

<p><h3>Next Post - Introduction to Word2Vec2Graph Model</h3>
In the next post we will introduce Word2Vec2Graph model - a combination of Word2Vec model and Graphs. We will build the model in Spark Machine Learning Library and Spark GraphFrame library.
</p>


                <hr>

                <ul class="pager">
                    
                    
                    <li class="next">
                        <a href="/2017/09/17/word2vec2graph/" data-toggle="tooltip" data-placement="top" title="Introduction to Word2Vec2Graph Model">Next Post &rarr;</a>
                    </li>
                    
                </ul>

            </div>
        </div>
    </div>
</article>

<hr>


    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    <li>
                        <a href="mailto:sparkling.dataocean@gmail.com">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <p class="copyright text-muted">Copyright &copy; Graph AI Studio 2026</p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>


    
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-114694347-1', 'auto');
  ga('send', 'pageview');

</script>



</body>

</html>
